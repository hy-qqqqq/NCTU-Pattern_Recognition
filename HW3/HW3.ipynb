{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW3: Decision Tree, AdaBoost and Random Forest\n",
    "In hw3, you need to implement decision tree, adaboost and random forest by using only numpy, then train your implemented model by the provided dataset and test the performance with testing data\n",
    "\n",
    "Please note that only **NUMPY** can be used to implement your model, you will get no points by simply calling sklearn.tree.DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data\n",
    "The dataset is the Heart Disease Data Set from UCI Machine Learning Repository. It is a binary classifiation dataset, the label is stored in `target` column. **Please note that there exist categorical features which need to be [one-hot encoding](https://www.datacamp.com/community/tutorials/categorical-data) before fit into your model!**\n",
    "See follow links for more information\n",
    "https://archive.ics.uci.edu/ml/datasets/heart+Disease"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "file_url = \"http://storage.googleapis.com/download.tensorflow.org/data/heart.csv\"\n",
    "df = pd.read_csv(file_url)\n",
    "\n",
    "train_idx = np.load('train_idx.npy')\n",
    "test_idx = np.load('test_idx.npy')\n",
    "\n",
    "train_df = df.iloc[train_idx]\n",
    "test_df = df.iloc[test_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>cp</th>\n",
       "      <th>trestbps</th>\n",
       "      <th>chol</th>\n",
       "      <th>fbs</th>\n",
       "      <th>restecg</th>\n",
       "      <th>thalach</th>\n",
       "      <th>exang</th>\n",
       "      <th>oldpeak</th>\n",
       "      <th>slope</th>\n",
       "      <th>ca</th>\n",
       "      <th>thal</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>54</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>192</td>\n",
       "      <td>283</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>195</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>reversible</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232</th>\n",
       "      <td>58</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>170</td>\n",
       "      <td>225</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>146</td>\n",
       "      <td>1</td>\n",
       "      <td>2.8</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>fixed</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>233</th>\n",
       "      <td>56</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>130</td>\n",
       "      <td>221</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>163</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>reversible</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>46</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>120</td>\n",
       "      <td>249</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>144</td>\n",
       "      <td>0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>reversible</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>55</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>135</td>\n",
       "      <td>250</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>161</td>\n",
       "      <td>0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>normal</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     age  sex  cp  trestbps  chol  fbs  restecg  thalach  exang  oldpeak  \\\n",
       "136   54    1   2       192   283    0        2      195      0      0.0   \n",
       "232   58    0   4       170   225    1        2      146      1      2.8   \n",
       "233   56    1   2       130   221    0        2      163      0      0.0   \n",
       "184   46    1   4       120   249    0        2      144      0      0.8   \n",
       "84    55    0   2       135   250    0        2      161      0      1.4   \n",
       "\n",
       "     slope  ca        thal  target  \n",
       "136      1   1  reversible       0  \n",
       "232      2   2       fixed       1  \n",
       "233      1   0  reversible       0  \n",
       "184      1   0  reversible       0  \n",
       "84       2   0      normal       0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n"
     ]
    }
   ],
   "source": [
    "print(train_df.to_numpy().shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1\n",
    "Gini Index or Entropy is often used for measuring the “best” splitting of the data. Please compute the Entropy and Gini Index of provided data. Please use the formula from [page 5 of hw3 slides](https://docs.google.com/presentation/d/1kIe_-YZdemRMmr_3xDy-l0OS2EcLgDH7Uan14tlU5KE/edit#slide=id.gd542a5ff75_0_15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gini(sequence, weights=None):\n",
    "    # the smaller, the purer\n",
    "    values, counts = np.unique(sequence, return_counts=True)\n",
    "    if weights is None:\n",
    "        probs = counts / len(sequence)\n",
    "    elif len(weights) == 0 or np.all(weights == weights[0]):\n",
    "        probs = counts / len(sequence)\n",
    "    else:\n",
    "        probs = []\n",
    "        for v in values:\n",
    "            idxs = np.where(sequence==v)[0]\n",
    "            probs.append(sum(weights[idxs]))\n",
    "    \n",
    "    return 1 - sum(p*p for p in probs)\n",
    "\n",
    "def entropy(sequence, weights=None):\n",
    "    # the smaller, the purer\n",
    "    values, counts = np.unique(sequence, return_counts=True)\n",
    "    if weights is None:\n",
    "        probs = counts / len(sequence)\n",
    "    elif len(weights) == 0 or np.all(weights == weights[0]):\n",
    "        probs = counts / len(sequence)\n",
    "    else:\n",
    "        probs = []\n",
    "        for v in values:\n",
    "            idxs = np.where(sequence==v)[0]\n",
    "            probs.append(sum(weights[idxs]))\n",
    "            print(v, idxs)\n",
    "        print(probs)\n",
    "    return -sum(p*np.log2(p) for p in probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 = class 1\n",
    "# 2 = class 2\n",
    "data = np.array([1,2,1,1,1,1,2,2,1,1,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gini of data is  0.4628099173553719\n"
     ]
    }
   ],
   "source": [
    "print(\"Gini of data is \", gini(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entropy of data is  0.9456603046006401\n"
     ]
    }
   ],
   "source": [
    "print(\"Entropy of data is \", entropy(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2\n",
    "Implement the Decision Tree algorithm (CART, Classification and Regression Trees) and trained the model by the given arguments, and print the accuracy score on the test data. You should implement two arguments for the Decision Tree algorithm\n",
    "1. **criterion**: The function to measure the quality of a split. Your model should support `gini` for the Gini impurity and `entropy` for the information gain. \n",
    "2. **max_depth**: The maximum depth of the tree. If `max_depth=None`, then nodes are expanded until all leaves are pure. `max_depth=1` equals to split data once\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_score(ytest: np.ndarray, ypred: np.ndarray):\n",
    "    return np.sum(np.equal(ytest, ypred)) / len(ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(X: np.ndarray, y: np.ndarray, threshold: int, feature_idx=None, weights=None):\n",
    "    if feature_idx != None:\n",
    "        splitpoint = X[:, feature_idx] <= threshold\n",
    "    else:\n",
    "        splitpoint = X <= threshold\n",
    "    # split X, y\n",
    "    Xleft, yleft = X[splitpoint], y[splitpoint]\n",
    "    Xright, yright = X[~splitpoint], y[~splitpoint]\n",
    "    # split w\n",
    "    if weights is not None:\n",
    "        wleft, wright = weights[splitpoint], weights[~splitpoint]\n",
    "    else:\n",
    "        wleft, wright = None, None\n",
    "    \n",
    "    return Xleft, Xright, yleft, yright, wleft, wright"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, impurity, amount, predicted):\n",
    "        self.impurity = impurity\n",
    "        self.amount = amount\n",
    "        self.predicted = predicted\n",
    "        self.feature_idx = -1\n",
    "        self.threshold = 0\n",
    "        self.left = None\n",
    "        self.right = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTree():\n",
    "    def __init__(self, criterion='gini', max_depth=None, random_best_split=False):\n",
    "        self.criterion = criterion\n",
    "        self.max_depth = max_depth\n",
    "        self.random_best_split = random_best_split\n",
    "        self.root = None # root node of the tree\n",
    "        # choose criterion function\n",
    "        if self.criterion not in ['gini', 'entropy']:\n",
    "            raise TypeError('criterion')\n",
    "        self.impurity = gini if self.criterion == 'gini' else entropy\n",
    "\n",
    "        return None\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f'DecisionTree(criterion={self.criterion}, max_depth={self.max_depth})'\n",
    "\n",
    "    def print_tree(self, cur: Node):\n",
    "        if cur == None:\n",
    "            return\n",
    "        print(cur.feature_idx, cur.threshold)\n",
    "        self.print_tree(cur.left)\n",
    "        self.print_tree(cur.right)\n",
    "\n",
    "    def _best_split_1(self, X: np.ndarray, y:np.ndarray, weights=None):\n",
    "        # initial\n",
    "        feature_idx = -1\n",
    "        threshold = None\n",
    "        min_impure = 1 # set to max possible value\n",
    "        # random attributes if true\n",
    "        attr_idxs = np.arange(X.shape[1])\n",
    "        if self.random_best_split:\n",
    "            attr_size = np.random.randint(X.shape[1])\n",
    "            attr_idxs = np.random.randint(X.shape[1], size=attr_size)\n",
    "        # loop all features\n",
    "        for i in attr_idxs:\n",
    "            x = np.array(X[:, i]) # i-th column of X\n",
    "            base = np.unique(x) # get unique values of  the column\n",
    "            # loop all possible thresholds\n",
    "            for j in range(len(base) - 1):\n",
    "                # j-th threshold is the average of the j-th and (j+1)-th\n",
    "                j_thre = (base[j]+base[j+1]) / 2\n",
    "                # get split arrays\n",
    "                _, _, yleft, yright, wleft, wright = split(x, y, threshold=j_thre, weights=weights)\n",
    "                # calculate weighted average of the children\n",
    "                impure_avg = (self.impurity(yleft, wleft)*(len(yleft)/len(y))) + (self.impurity(yright, wright)*(len(yright)/len(y)))\n",
    "                # update\n",
    "                if impure_avg < min_impure:\n",
    "                    feature_idx = i\n",
    "                    threshold = j_thre\n",
    "                    min_impure = impure_avg\n",
    "        # return\n",
    "        return feature_idx, threshold\n",
    "\n",
    "    def _best_split_2(self, X: np.ndarray, y:np.ndarray, weights=None):\n",
    "        # initial\n",
    "        feature_idx = -1\n",
    "        threshold = None\n",
    "        min_impure = 1 # set to max possible value\n",
    "        # random attributes if true\n",
    "        attr_idxs = np.arange(X.shape[1])\n",
    "        if self.random_best_split:\n",
    "            attr_size = np.random.randint(X.shape[1])\n",
    "            attr_idxs = np.random.randint(X.shape[1], size=attr_size)\n",
    "        # loop all features\n",
    "        for i in attr_idxs:\n",
    "            x = np.array(X[:, i]) # i-th column of X\n",
    "            base = np.unique(x) # get unique values of  the column\n",
    "            # loop all possible thresholds\n",
    "            for j in base:\n",
    "                # get split arrays\n",
    "                _, _, yleft, yright, wleft, wright = split(x, y, threshold=j, weights=weights)\n",
    "                # calculate weighted average of the children\n",
    "                impure_avg = (self.impurity(yleft, wleft)*(len(yleft)/len(y))) + (self.impurity(yright, wright)*(len(yright)/len(y)))\n",
    "                # update\n",
    "                if impure_avg < min_impure:\n",
    "                    feature_idx = i\n",
    "                    threshold = j\n",
    "                    min_impure = impure_avg\n",
    "        # return\n",
    "        return feature_idx, threshold\n",
    "\n",
    "    def fit(self, X: np.ndarray, y: np.ndarray, sample_weight=None):\n",
    "        # starts with the whole data\n",
    "        self.root = self._grow(X, y, sample_weight)\n",
    "        # calculate feature importance from nodes\n",
    "        self.feature_importance = np.zeros(X.shape[1], int)\n",
    "        self._calc_feature_importance(self.root)\n",
    "        return self\n",
    "\n",
    "    def _grow(self, X: np.ndarray, y:np.ndarray, weights: np.ndarray, depth=0):\n",
    "        # empty node\n",
    "        if X.size == 0:\n",
    "            return None\n",
    "        # create a node (calculate impurity with weights if needed)\n",
    "        vals, cnts = np.unique(y, return_counts=True)\n",
    "        node = Node(impurity=self.impurity(y, weights=weights), amount=len(y), predicted=vals[np.argmax(cnts)])\n",
    "        # node is pure or reach max depth\n",
    "        if node.impurity == 0:\n",
    "            return node\n",
    "        if (self.max_depth is not None) and (depth >= self.max_depth):\n",
    "            return node\n",
    "        # keep building tree, loop all features to find best partition\n",
    "        feature_idx, threshold = self._best_split_2(X, y, weights)\n",
    "        # split (exists better split)\n",
    "        if feature_idx != -1:\n",
    "            Xleft, Xright, yleft, yright, wleft, wright = split(X, y, threshold=threshold, feature_idx=feature_idx, weights=weights)\n",
    "            node.feature_idx = feature_idx\n",
    "            node.threshold = threshold\n",
    "            # grow children\n",
    "            node.left = self._grow(Xleft, yleft, wleft, depth+1)\n",
    "            node.right = self._grow(Xright, yright, wright, depth+1)\n",
    "        # return current node\n",
    "        return node\n",
    "    \n",
    "    def predict(self, X: np.ndarray):\n",
    "        if self.root == None:\n",
    "            raise RuntimeError('fit the model first')\n",
    "        ypred = []\n",
    "        for datapoint in X:\n",
    "            node = self.root\n",
    "            while node.left != None and node.right != None:\n",
    "                if node.feature_idx == -1:\n",
    "                    break\n",
    "                node = node.left if datapoint[node.feature_idx] <= node.threshold else node.right\n",
    "            ypred.append(node.predicted)\n",
    "\n",
    "        return ypred\n",
    "    \n",
    "    def _calc_feature_importance(self, node: Node):\n",
    "        if node.feature_idx != -1:\n",
    "            cur_level_fi = node.amount * node.impurity\n",
    "            # decreased with children impurity * amount\n",
    "            if node.left != None:\n",
    "                left_fi = self._calc_feature_importance(node.left)\n",
    "                cur_level_fi -= left_fi\n",
    "            if node.right != None:\n",
    "                right_fi = self._calc_feature_importance(node.right)\n",
    "                cur_level_fi -= right_fi\n",
    "            \n",
    "            self.feature_importance[node.feature_idx] += cur_level_fi\n",
    "\n",
    "            return node.amount * node.impurity\n",
    "\n",
    "        return 0\n",
    "    \n",
    "    def get_feature_importance(self):\n",
    "        if self.root == None:\n",
    "            raise RuntimeError('fit the model first')\n",
    "        return self.feature_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "int64\n",
      "int64\n"
     ]
    }
   ],
   "source": [
    "# transform values to numeric (thal)\n",
    "train_df = train_df.replace({'thal': {'normal': 3, 'fixed': 6, 'reversible': 7}})\n",
    "test_df = test_df.replace({'thal': {'normal': 3, 'fixed': 6, 'reversible': 7}})\n",
    "# check rows with bad value\n",
    "print(train_df['thal'].dtype)\n",
    "print(test_df['thal'].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split x, y and turn into numpy array\n",
    "Xtrain = train_df.drop(columns=['target']).to_numpy()\n",
    "ytrain = train_df['target'].to_numpy()\n",
    "Xtest = test_df.drop(columns=['target']).to_numpy()\n",
    "ytest = test_df['target'].to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2.1\n",
    "Using `criterion=gini`, showing the accuracy score of test data by `max_depth=3` and `max_depth=10`, respectively.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of DecisionTree(criterion=gini, max_depth=3):\n",
      "0.79\n"
     ]
    }
   ],
   "source": [
    "# get decision tree instance\n",
    "clf_depth3 = DecisionTree(criterion='gini', max_depth=3)\n",
    "# fit train data into model\n",
    "clf_depth3 = clf_depth3.fit(Xtrain, ytrain)\n",
    "# predict with test data\n",
    "ypred = clf_depth3.predict(Xtest)\n",
    "# calculate accuracy score and output (should be larger than 0.7)\n",
    "print(f'Accuracy of {clf_depth3}:')\n",
    "print(accuracy_score(ytest, ypred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of DecisionTree(criterion=gini, max_depth=10):\n",
      "0.75\n"
     ]
    }
   ],
   "source": [
    "# get decision tree instance\n",
    "clf_depth10 = DecisionTree(criterion='gini', max_depth=10)\n",
    "# fit train data into model\n",
    "clf_depth10 = clf_depth10.fit(Xtrain, ytrain)\n",
    "# predict with test data\n",
    "ypred = clf_depth10.predict(Xtest)\n",
    "# calculate accuracy score and output (should be larger than 0.7)\n",
    "print(f'Accuracy of {clf_depth10}:')\n",
    "print(accuracy_score(ytest, ypred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2.2\n",
    "Using `max_depth=3`, showing the accuracy score of test data by `criterion=gini` and `criterion=entropy`, respectively.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Note: Your decisition tree scores should over **0.7**. It may suffer from overfitting, if so, you can tune the hyperparameter such as `max_depth`\n",
    "- Note: You should get the same results when re-building the model with the same arguments,  no need to prune the trees\n",
    "- Hint: You can use the recursive method to build the nodes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of DecisionTree(criterion=gini, max_depth=3):\n",
      "0.79\n"
     ]
    }
   ],
   "source": [
    "# get decision tree instance\n",
    "clf_gini = DecisionTree(criterion='gini', max_depth=3)\n",
    "# fit train data into model\n",
    "clf_gini = clf_gini.fit(Xtrain, ytrain)\n",
    "# predict with test data\n",
    "ypred = clf_gini.predict(Xtest)\n",
    "# calculate accuracy score and output (should be larger than 0.7)\n",
    "print(f'Accuracy of {clf_gini}:')\n",
    "print(accuracy_score(ytest, ypred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of DecisionTree(criterion=entropy, max_depth=3):\n",
      "0.76\n"
     ]
    }
   ],
   "source": [
    "# get decision tree instance\n",
    "clf_entropy = DecisionTree(criterion='entropy', max_depth=3)\n",
    "# fit train data into model\n",
    "clf_entropy = clf_entropy.fit(Xtrain, ytrain)\n",
    "# predict with test data\n",
    "ypred = clf_entropy.predict(Xtest)\n",
    "# calculate accuracy score and output (should be larger than 0.7)\n",
    "print(f'Accuracy of {clf_entropy}:')\n",
    "print(accuracy_score(ytest, ypred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3\n",
    "Plot the [feature importance](https://sefiks.com/2020/04/06/feature-importance-in-decision-trees/) of your Decision Tree model. You can get the feature importance by counting the feature used for splitting data.\n",
    "\n",
    "- You can simply plot the **counts of feature used** for building tree without normalize the importance. Take the figure below as example, outlook feature has been used for splitting for almost 50 times. Therefore, it has the largest importance\n",
    "\n",
    "![image](https://i2.wp.com/sefiks.com/wp-content/uploads/2020/04/c45-fi-results.jpg?w=481&ssl=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only for plotting figures\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1008x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# using the model from question 2.1 with max depth = 10\n",
    "feature_importance = clf_depth10.get_feature_importance()\n",
    "feature_name = np.array(train_df.columns)\n",
    "feature_name = feature_name[:len(feature_name)-1]\n",
    "# sort\n",
    "p = feature_importance.argsort()[::-1]\n",
    "# plot\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.bar(feature_name[p], feature_importance[p])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4\n",
    "implement the AdaBooest algorithm by using the CART you just implemented from question 2 as base learner. You should implement one arguments for the AdaBooest.\n",
    "1. **n_estimators**: The maximum number of estimators at which boosting is terminated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaBoost():\n",
    "    def __init__(self, n_estimators):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.strong_clf = None\n",
    "        return None\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f'AdaBoost(n_estimators={self.n_estimators})'\n",
    "\n",
    "    def fit(self, X: np.ndarray, y_pretransform: np.ndarray):\n",
    "        y = np.copy(y_pretransform)\n",
    "        # turn target y into -1s and 1s if needed\n",
    "        uniq = np.unique(y_pretransform)\n",
    "        if not np.all(uniq==np.array([-1, 1])):\n",
    "            y[y==0] = -1\n",
    "        # initialize weight distribution\n",
    "        weights = np.full(len(y), 1/float(len(y)))\n",
    "        # store the fitted weak clfs\n",
    "        self.strong_clf = []\n",
    "        for _ in range(self.n_estimators):\n",
    "            # fit weak classifier with sample weight\n",
    "            weak_clf = DecisionTree(criterion='gini', max_depth=3)\n",
    "            weak_clf = weak_clf.fit(X, y, sample_weight=weights)\n",
    "            ypred = weak_clf.predict(X)\n",
    "            # calculate weighted error (epsilon)\n",
    "            diff = np.not_equal(y, ypred)\n",
    "            idxs = np.where(diff == True)[0]\n",
    "            weighted_error = sum(weights[idxs]) / sum(weights)\n",
    "            # calculate alpha\n",
    "            alpha = 0.5 * np.log((1-weighted_error)/float(weighted_error))\n",
    "            # update weights (not normalized)\n",
    "            weights = weights * np.exp(-alpha*y*ypred)\n",
    "            # update strong clf\n",
    "            self.strong_clf.append((alpha, weak_clf))\n",
    "        \n",
    "        return self\n",
    "\n",
    "    def predict(self, X: np.ndarray):\n",
    "        if self.strong_clf == None:\n",
    "            raise RuntimeError('fit the model first')\n",
    "        # initial ypred\n",
    "        ypred = np.zeros(X.shape[0])\n",
    "        # get predictions from strong clf\n",
    "        for alpha, weak_clf in self.strong_clf:\n",
    "            preds = weak_clf.predict(X)\n",
    "            ypred += alpha*np.array(preds)\n",
    "        ypred = np.sign(ypred).astype(int)\n",
    "        # reset y values to original\n",
    "        ypred[ypred==-1] = 0\n",
    "        return ypred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4.1\n",
    "Show the accuracy score of test data by `n_estimators=10` and `n_estimators=100`, respectively.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of AdaBoost(n_estimators=10):\n",
      "0.79\n"
     ]
    }
   ],
   "source": [
    "clf_10adaboost = AdaBoost(n_estimators=10)\n",
    "clf_10adaboost = clf_10adaboost.fit(Xtrain, ytrain)\n",
    "ypred = clf_10adaboost.predict(Xtest)\n",
    "print(f'Accuracy of {clf_10adaboost}:')\n",
    "print(accuracy_score(ytest, ypred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of AdaBoost(n_estimators=100):\n",
      "0.79\n"
     ]
    }
   ],
   "source": [
    "clf_100adaboost = AdaBoost(n_estimators=100)\n",
    "clf_100adaboost = clf_100adaboost.fit(Xtrain, ytrain)\n",
    "ypred = clf_100adaboost.predict(Xtest)\n",
    "print(f'Accuracy of {clf_100adaboost}:')\n",
    "print(accuracy_score(ytest, ypred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5\n",
    "implement the Random Forest algorithm by using the CART you just implemented from question 2. You should implement three arguments for the Random Forest.\n",
    "\n",
    "1. **n_estimators**: The number of trees in the forest. \n",
    "2. **max_features**: The number of random select features to consider when looking for the best split\n",
    "3. **bootstrap**: Whether bootstrap samples are used when building tree\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomForest():\n",
    "    def __init__(self, n_estimators, max_features, bootstrap=True, criterion='gini', max_depth=None):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.max_features = max_features\n",
    "        self.bootstrap = bootstrap\n",
    "        self.criterion = criterion\n",
    "        self.max_depth = max_depth\n",
    "        self.trees = None\n",
    "        return None\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f'RandomForest(n_estimators={self.n_estimators}, max_features={self.max_features})'\n",
    "    \n",
    "    def fit(self, X: np.ndarray, y: np.ndarray, random_seed=None):\n",
    "        # collect trees to become a forest\n",
    "        self.trees = []\n",
    "        # for every bootstrap dataset, build one decision tree\n",
    "        np.random.seed(random_seed)\n",
    "        for _ in range(self.n_estimators):\n",
    "            # random sample index to build a bootstrap dataset\n",
    "            idxs = np.random.randint(X.shape[0], size=X.shape[0])\n",
    "            X_bootstrap, y_bootstrap = X[idxs], y[idxs]\n",
    "            # decision tree instance\n",
    "            tree_clf = DecisionTree(criterion=self.criterion, max_depth=self.max_depth, random_best_split=True)\n",
    "            tree_clf = tree_clf.fit(X_bootstrap, y_bootstrap)\n",
    "            self.trees.append(tree_clf)\n",
    "\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X: np.ndarray):\n",
    "        if self.trees is None:\n",
    "            raise RuntimeError('fit the model first')\n",
    "        # initial ypred\n",
    "        votes = []\n",
    "        # get predictions from trees\n",
    "        for tree_clf in self.trees:\n",
    "            preds = tree_clf.predict(X)\n",
    "            votes.append(preds)\n",
    "        votes = np.array(votes)\n",
    "        # get majority vote\n",
    "        ypred = []\n",
    "        for i in range(votes.shape[1]):\n",
    "            vals, cnts = np.unique(votes[:, i], return_counts=True)\n",
    "            ypred.append(vals[np.argmax(cnts)])\n",
    "\n",
    "        return ypred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5.1\n",
    "Using `criterion=gini`, `max_depth=None`, `max_features=sqrt(n_features)`, showing the accuracy score of test data by `n_estimators=10` and `n_estimators=100`, respectively.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_10tree = RandomForest(n_estimators=10, max_features=np.sqrt(Xtrain.shape[1]))\n",
    "clf_10tree = clf_10tree.fit(Xtrain, ytrain, random_seed=11000)\n",
    "ypred = clf_10tree.predict(Xtest)\n",
    "accuracy_score(ytest, ypred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.82"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_100tree = RandomForest(n_estimators=100, max_features=np.sqrt(Xtrain.shape[1]))\n",
    "clf_100tree = clf_100tree.fit(Xtrain, ytrain, random_seed=12000)\n",
    "ypred = clf_100tree.predict(Xtest)\n",
    "accuracy_score(ytest, ypred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5.2\n",
    "Using `criterion=gini`, `max_depth=None`, `n_estimators=10`, showing the accuracy score of test data by `max_features=sqrt(n_features)` and `max_features=n_features`, respectively.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.83"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_random_features = RandomForest(n_estimators=10, max_features=np.sqrt(Xtrain.shape[1]))\n",
    "clf_random_features = clf_random_features.fit(Xtrain, ytrain)\n",
    "ypred = clf_random_features.predict(Xtest)\n",
    "accuracy_score(ytest, ypred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Note: Use majority votes to get the final prediction, you may get slightly different results when re-building the random forest model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.82"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_all_features = RandomForest(n_estimators=10, max_features=Xtrain.shape[1])\n",
    "clf_all_features = clf_all_features.fit(Xtrain, ytrain)\n",
    "ypred = clf_all_features.predict(Xtest)\n",
    "accuracy_score(ytest, ypred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6.\n",
    "Try you best to get highest test accuracy score by \n",
    "- Feature engineering\n",
    "- Hyperparameter tuning\n",
    "- Implement any other ensemble methods, such as gradient boosting. Please note that you cannot call any package. Also, only ensemble method can be used. Neural network method is not allowed to used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'your_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-123-68c908e25463>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0myour_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'your_model' is not defined"
     ]
    }
   ],
   "source": [
    "y_pred = your_model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Test-set accuarcy score: ', accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supplementary\n",
    "If you have trouble to implement this homework, TA strongly recommend watching [this video](https://www.youtube.com/watch?v=LDRbO9a6XPU), which explains Decision Tree model clearly. But don't copy code from any resources, try to finish this homework by yourself! "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
